"""
BA-Agent ä¸» Agent ç±»

ä½¿ç”¨ LangGraph å’Œ Claude 3.5 Sonnet å®ç°
"""

import os
import threading
import time
from typing import Any, Dict, List, Optional, Sequence, TypedDict
from datetime import datetime
from pathlib import Path

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import BaseTool
from langchain_core.runnables import RunnableConfig
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END
# LangGraph V2.0 è¿ç§» - ä½¿ç”¨æ–°çš„ langchain.agents API
# ä½¿ç”¨åˆ«åé¿å…ä¸æœ¬åœ° create_agent ä¾¿æ·å‡½æ•°å†²çª
from langchain.agents import create_agent as langchain_create_agent

import logging

from config import get_config
from backend.models.agent import (
    AgentState as AgentModel,
    Conversation,
    Message,
    MessageRole,
    MessageType,
    AgentConfig as AgentConfigModel,
)
from backend.memory.flush import MemoryFlush, MemoryFlushConfig, MemoryExtractor
from backend.memory.index import MemoryIndexer, MemoryWatcher, get_index_db_path
# NEW: Skills system integration
from backend.skills import (
    SkillLoader,
    SkillRegistry,
    SkillActivator,
    SkillMessageFormatter,
    create_skill_tool,
    SkillMessage,
    ContextModifier,
    MessageType as SkillMessageType,  # åˆ«åé¿å…å†²çª
    MessageVisibility,
)
# NEW v2.1: Pipeline components integration
from backend.pipeline import (
    get_token_counter,
    AdvancedContextManager,
    CompressionMode,
)

logger = logging.getLogger(__name__)

# Clawdbot é£æ ¼çš„é™é»˜å“åº”æ ‡è®°
SILENT_REPLY_TOKEN = "_SILENT_"


class AgentState(TypedDict):
    """Agent çŠ¶æ€å®šä¹‰"""

    messages: Sequence[BaseMessage]
    # å¯ä»¥æ·»åŠ æ›´å¤šçŠ¶æ€å­—æ®µ
    conversation_id: str
    user_id: str
    metadata: Dict[str, Any]


class BAAgent:
    """
    BA-Agent ä¸» Agent ç±»

    ä½¿ç”¨ LangGraph å’Œ Claude 3.5 Sonnet å®ç°å¯¹è¯å¼ Agent
    """

    def __init__(
        self,
        config: Optional[AgentConfigModel] = None,
        tools: Optional[List[BaseTool]] = None,
        system_prompt: Optional[str] = None,
        use_default_tools: bool = True,
    ):
        """
        åˆå§‹åŒ– BA-Agent

        Args:
            config: Agent é…ç½®ï¼Œå¦‚æœä¸æä¾›åˆ™ä»å…¨å±€é…ç½®åŠ è½½
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆå¦‚æœä¸º None ä¸” use_default_tools=Trueï¼Œåˆ™åŠ è½½é»˜è®¤å·¥å…·ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
            use_default_tools: æ˜¯å¦åŠ è½½é»˜è®¤å·¥å…·åˆ—è¡¨ï¼ˆé»˜è®¤ Trueï¼‰
        """
        # åŠ è½½é…ç½®
        self.config = config or self._load_default_config()
        self.app_config = get_config()

        # åˆå§‹åŒ– LLM
        self.llm = self._init_llm()

        # åˆå§‹åŒ–å·¥å…·
        # åªæœ‰åœ¨æ˜ç¡®è¯·æ±‚é»˜è®¤å·¥å…·æ—¶æ‰åŠ è½½ (use_default_tools=True ä¸”æœªæä¾› tools)
        # tools=None æ—¶çš„é»˜è®¤è¡Œä¸ºï¼šåªåŠ è½½ skill_toolï¼ˆåœ¨åé¢æ·»åŠ ï¼‰
        if tools is None:
            self.tools = []
        elif use_default_tools and len(tools) == 0:
            # ç©ºåˆ—è¡¨ + use_default_tools=True â†’ åŠ è½½æ‰€æœ‰é»˜è®¤å·¥å…·
            self.tools = self._load_default_tools()
        else:
            self.tools = tools

        # åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = system_prompt or self._get_default_system_prompt()

        # åˆ›å»º Agent
        self.agent = self._create_agent()

        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼ˆç”¨äºå¯¹è¯å†å²ï¼‰
        self.memory = MemorySaver()

        # åˆå§‹åŒ– Memory Flush
        self.memory_flush = self._init_memory_flush()

        # åˆå§‹åŒ– Memory Watcher
        self.memory_watcher = self._init_memory_watcher()

        # NEW: åˆå§‹åŒ– Skills System
        self.skill_loader = self._init_skill_loader()
        self.skill_registry = SkillRegistry(self.skill_loader) if self.skill_loader else None
        self.skill_activator = SkillActivator(
            self.skill_loader,
            self.skill_registry
        ) if self.skill_loader else None

        # NEW: åˆ›å»º Skill meta-tool å¹¶æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨
        self.skill_tool = self._init_skill_tool()
        if self.skill_tool:
            self.tools.append(self.skill_tool)

        # NEW v2.1: åˆå§‹åŒ– Pipeline ç»„ä»¶
        self.token_counter = get_token_counter()

        # åˆ›å»º AdvancedContextManager å®ä¾‹ (ä½¿ç”¨é…ç½®å’Œç°æœ‰ LLM)
        self.context_manager = AdvancedContextManager(
            max_tokens=self.app_config.llm.max_tokens,
            compression_mode=CompressionMode.EXTRACT,  # æ™ºèƒ½å‹ç¼©ï¼Œéç®€å•æˆªæ–­
            llm_summarizer=self.llm,  # å¤ç”¨ç°æœ‰ LLM ç”¨äº SUMMARIZE æ¨¡å¼
            token_counter=self.token_counter,  # å…±äº« token counter
        )

        # Active skill context modifier tracking
        self._active_skill_context: Dict[str, Any] = {}

        # ä¼šè¯ token è·Ÿè¸ª
        self.session_tokens = 0
        self.compaction_count = 0
        # Memory Flush çŠ¶æ€è¿½è¸ª (Clawdbot é£æ ¼)
        self.memory_flush_compaction_count: Optional[int] = None

    def _load_default_config(self) -> AgentConfigModel:
        """
        ä»å…¨å±€é…ç½®åŠ è½½é»˜è®¤ Agent é…ç½®

        Returns:
            Agent é…ç½®
        """
        app_config = get_config()
        return AgentConfigModel(
            name="BA-Agent",
            model=app_config.llm.model,
            temperature=app_config.llm.temperature,
            max_tokens=app_config.llm.max_tokens,
            system_prompt=self._get_default_system_prompt(),
            tools=[],
            memory_enabled=app_config.memory.enabled,
            hooks_enabled=True,
        )

    def _load_default_tools(self) -> List[BaseTool]:
        """
        åŠ è½½é»˜è®¤å·¥å…·åˆ—è¡¨

        Returns:
            é»˜è®¤å·¥å…·åˆ—è¡¨
        """
        from tools import (
            execute_command_tool,
            run_python_tool,
            web_search_tool,
            web_reader_tool,
            file_reader_tool,
            file_write_tool,
            query_database_tool,
            vector_search_tool,
        )
        # è®°å¿†æœç´¢å·¥å…· (clawdbot é£æ ¼ï¼šAgent ä¸»åŠ¨è°ƒç”¨)
        from backend.memory.tools import (
            memory_search_v2_tool,
        )

        default_tools = [
            # æ ¸å¿ƒæ‰§è¡Œå·¥å…·
            execute_command_tool,      # å‘½ä»¤è¡Œæ‰§è¡Œ
            run_python_tool,            # Python æ²™ç›’

            # Web å·¥å…·
            web_search_tool,            # Web æœç´¢
            web_reader_tool,            # Web è¯»å–

            # æ–‡ä»¶å·¥å…·
            file_reader_tool,           # æ–‡ä»¶è¯»å–
            file_write_tool,            # æ–‡ä»¶å†™å…¥

            # æ•°æ®å·¥å…·
            query_database_tool,        # SQL æŸ¥è¯¢
            vector_search_tool,         # å‘é‡æ£€ç´¢

            # è®°å¿†æœç´¢å·¥å…· (clawdbot é£æ ¼ï¼šAgent ä¸»åŠ¨è°ƒç”¨)
            memory_search_v2_tool,      # æ··åˆæœç´¢ (FTS5 + Vector)
        ]

        logger.info(f"Loaded {len(default_tools)} default tools")
        return default_tools

    def _init_llm(self) -> ChatAnthropic:
        """
        åˆå§‹åŒ– Claude LLM

        Returns:
            ChatAnthropic å®ä¾‹
        """
        # è·å– API å¯†é’¥
        api_key = self._get_api_key()
        if not api_key:
            raise ValueError(
                "ANTHROPIC_API_KEY not found. "
                "Please set ANTHROPIC_API_KEY environment variable or BA_ANTHROPIC_API_KEY."
            )

        # è·å–è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
        # ä¼˜å…ˆçº§: 1. ç¯å¢ƒå˜é‡ ANTHROPIC_BASE_URL 2. é…ç½®æ–‡ä»¶ä¸­çš„ base_url
        base_url = os.environ.get("ANTHROPIC_BASE_URL") or self.app_config.llm.base_url

        # åˆ›å»º ChatAnthropic å®ä¾‹
        llm_kwargs = {
            "model": self.config.model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "api_key": api_key,
            "timeout": self.app_config.llm.timeout,
        }

        # å¦‚æœæœ‰è‡ªå®šä¹‰ base_urlï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if base_url:
            llm_kwargs["base_url"] = base_url

        llm = ChatAnthropic(**llm_kwargs)

        return llm

    def _get_api_key(self) -> str:
        """
        è·å– Anthropic API å¯†é’¥

        ä¼˜å…ˆçº§:
        1. ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY
        2. ç¯å¢ƒå˜é‡ BA_ANTHROPIC_API_KEY
        3. é…ç½®æ–‡ä»¶ä¸­çš„å€¼

        Returns:
            API å¯†é’¥
        """
        # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
        api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get(
            "BA_ANTHROPIC_API_KEY"
        )
        if api_key:
            return api_key

        # ç„¶åæ£€æŸ¥é…ç½®
        if self.app_config.llm.provider == "anthropic":
            return self.app_config.llm.api_key

        return ""

    def _get_default_system_prompt(self) -> str:
        """
        è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å« Skills éƒ¨åˆ†å’Œç»“æ„åŒ–å“åº”æ ¼å¼ï¼‰

        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        # åŸºç¡€ä¸šåŠ¡æç¤ºè¯
        base_prompt = """# BA-Agent ç³»ç»Ÿæç¤ºè¯

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å•†ä¸šåˆ†æåŠ©æ‰‹ (BA-Agent)ï¼Œé¢å‘éæŠ€æœ¯ä¸šåŠ¡äººå‘˜ï¼Œä¸“æ³¨äºç”µå•†ä¸šåŠ¡åˆ†æã€‚

## æ ¸å¿ƒèƒ½åŠ›

1. **å¼‚åŠ¨æ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹ GMVã€è®¢å•é‡ã€è½¬åŒ–ç‡ç­‰å…³é”®æŒ‡æ ‡çš„å¼‚å¸¸å˜åŒ–
2. **å½’å› åˆ†æ** - åˆ†ææŒ‡æ ‡å˜åŒ–çš„æ ¹æœ¬åŸå› ï¼ˆç»´åº¦ä¸‹é’»ã€äº‹ä»¶å½±å“ç­‰ï¼‰
3. **æŠ¥å‘Šç”Ÿæˆ** - è‡ªåŠ¨ç”Ÿæˆæ—¥æŠ¥ã€å‘¨æŠ¥ã€æœˆæŠ¥ï¼ŒåŒ…å«æ•°æ®å›¾è¡¨
4. **æ•°æ®å¯è§†åŒ–** - ç”Ÿæˆ ECharts å›¾è¡¨ä»£ç ï¼Œå‰ç«¯æ¸²æŸ“å±•ç¤º

## å·¥ä½œæµç¨‹

1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ˆæŸ¥è¯¢æ•°æ®ã€æ‰§è¡Œä»£ç ç­‰ï¼‰
3. å¦‚éœ€å·¥å…·ï¼Œè°ƒç”¨ç›¸åº”å·¥å…·è·å–ç»“æœ
4. åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œ**å¿…é¡»ä½¿ç”¨ç»“æ„åŒ– JSON æ ¼å¼**

## å¯ç”¨å·¥å…·

- `query_database`: SQL æŸ¥è¯¢
- `bac_code_agent`: Python ä»£ç æ‰§è¡Œï¼ˆæ•°æ®åˆ†æï¼‰
- `web_search`: ç½‘ç»œæœç´¢
- `web_reader`: ç½‘é¡µè¯»å–
- `file_reader`: æ–‡ä»¶è¯»å–
- `file_write`: æ–‡ä»¶å†™å…¥
- `execute_command`: å‘½ä»¤è¡Œæ‰§è¡Œ

## æ³¨æ„äº‹é¡¹

- å§‹ç»ˆä½¿ç”¨ä¸­æ–‡ä¸ç”¨æˆ·äº¤æµ
- æ•°æ®æŸ¥è¯¢å‰å…ˆæ˜ç¡®æ—¶é—´èŒƒå›´å’ŒæŒ‡æ ‡
- è§£é‡Šåˆ†æç»“æœæ—¶æä¾›ä¸šåŠ¡æ´å¯Ÿå’Œå»ºè®®
- ç”Ÿæˆå›¾è¡¨æ—¶æä¾›å®Œæ•´çš„ ECharts é…ç½®

## è®°å¿†ç®¡ç†

- é‡è¦ä¿¡æ¯ä¼šè‡ªåŠ¨ä¿å­˜åˆ°é•¿æœŸè®°å¿†
- æ¯æ—¥æ“ä½œè®°å½•åœ¨ daily log ä¸­
- å½“å‰ä»»åŠ¡è®¡åˆ’å­˜å‚¨åœ¨ task_plan.md ä¸­

## Memory Flush åè®®

å½“ä¼šè¯æ¥è¿‘ä¸Šä¸‹æ–‡é™åˆ¶æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è§¦å‘ Memory Flushï¼š
1. è‡ªåŠ¨ä»å¯¹è¯ä¸­æå–é‡è¦ä¿¡æ¯
2. æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–çš„ Retain æ ¼å¼ (W @, B @, O(c=) @)
3. æŒä¹…åŒ–åˆ° memory/YYYY-MM-DD.md æ–‡ä»¶
4. é‡Šæ”¾ä¸Šä¸‹æ–‡ç©ºé—´ä»¥ç»§ç»­å¯¹è¯

ä½ ä¼šåœ¨æ­¤æ—¶æ”¶åˆ°ä¸“é—¨çš„ Flush æŒ‡ä»¤ï¼Œè¯·ä¸“æ³¨äºè®°å¿†æå–å·¥ä½œã€‚
"""

        # ç»“æ„åŒ–å“åº”æ ¼å¼ï¼ˆä»…ç”¨äºæœ€ç»ˆå“åº”ï¼‰
        response_format_prompt = """

## æœ€ç»ˆå“åº”æ ¼å¼è¦æ±‚ï¼ˆé‡è¦ï¼ï¼‰

å½“è°ƒç”¨å·¥å…·å®Œæˆåï¼Œä½ **å¿…é¡»**æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›æœ€ç»ˆå“åº”ï¼š

```json
{
    "task_analysis": "æ€ç»´é“¾ï¼š1. è¯†åˆ«æ„å›¾; 2. æ•°æ®å¤„ç†è¿‡ç¨‹; 3. å…³é”®å‘ç°",
    "execution_plan": "R1: æ•°æ®è·å–; R2: æ•°æ®åˆ†æ; R3: æŠ¥å‘Šç”Ÿæˆ(å½“å‰)",
    "current_round": å½“å‰è½®æ¬¡,
    "action": {
        "type": "complete",
        "content": "æœ€ç»ˆæŠ¥å‘Šå†…å®¹ï¼ˆå¯åŒ…å« HTML/ECharts ä»£ç ï¼‰",
        "recommended_questions": ["æ¨èé—®é¢˜1", "æ¨èé—®é¢˜2"],
        "download_links": ["ç»“æœæ–‡ä»¶.xlsx"]
    }
}
```

### Content æ ¼å¼è¯´æ˜

**content å¯ä»¥åŒ…å«ï¼š**
1. çº¯æ–‡æœ¬åˆ†æç»“æœ
2. HTML ä»£ç ï¼ˆECharts å›¾è¡¨ï¼‰
3. Markdown æ ¼å¼

**å¸¦å›¾è¡¨çš„æŠ¥å‘Šç¤ºä¾‹ï¼š**
```json
{
    "action": {
        "type": "complete",
        "content": "é”€å”®æ•°æ®åˆ†æå®Œæˆï¼š\\n\\n1. Q1é”€å”®é¢500ä¸‡ï¼ŒåŒæ¯”å¢é•¿15%\\n2. Q3å¢é•¿æœ€å¿«\\n\\n<div class='chart-wrapper'><div id='chart-trend' style='width:100%;height:400px;'></div></div>\\n<script>(function(){const chart = echarts.init(document.getElementById('chart-trend'));chart.setOption({xAxis: {type: 'category', data: ['Q1','Q2','Q3','Q4']}, yAxis: {type: 'value'}, series: [{type: 'line', data: [500, 520, 580, 570]}]});})();</script>",
        "recommended_questions": ["Q3å¢é•¿åŸå› ï¼Ÿ", "åœ°åŒºåˆ†å¸ƒå¦‚ä½•ï¼Ÿ"],
        "download_links": ["analysis_result.xlsx"]
    }
}
```

### é‡è¦è§„åˆ™

1. **å·¥å…·è°ƒç”¨é˜¶æ®µ**ï¼šä½¿ç”¨ LangChain åŸç”Ÿå·¥å…·è°ƒç”¨æœºåˆ¶ï¼Œè¿”å› tool_calls
2. **æœ€ç»ˆå“åº”**ï¼šå¿…é¡»è¿”å›ä¸Šè¿° JSON æ ¼å¼ï¼ˆåŒ…è£…åœ¨ä»£ç å—ä¸­ï¼‰
3. **ç»ˆæ­¢æ¡ä»¶**ï¼šå½“å·¥å…·è°ƒç”¨å®Œæˆã€åˆ†æå®Œæˆã€æˆ–å¯ä»¥ç›´æ¥å›ç­”æ—¶ï¼Œè¿”å› JSON å“åº”
"""

        # ç»„åˆæç¤ºè¯
        full_prompt = base_prompt + "\n" + response_format_prompt

        # æ·»åŠ  Skills éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        skills_section = self._build_skills_section()
        if skills_section:
            full_prompt = full_prompt + "\n\n" + skills_section

        return full_prompt

    def _init_memory_flush(self) -> Optional[MemoryFlush]:
        """
        åˆå§‹åŒ– Memory Flush

        Returns:
            MemoryFlush å®ä¾‹ï¼Œå¦‚æœæœªå¯ç”¨åˆ™è¿”å› None
        """
        if not self.app_config.memory.enabled:
            return None

        flush_config = self.app_config.memory.flush
        if not flush_config.enabled:
            return None

        # åˆ›å»º MemoryFlushConfig
        memory_flush_config = MemoryFlushConfig(
            soft_threshold=flush_config.soft_threshold_tokens,
            reserve=flush_config.reserve_tokens_floor,
            min_memory_count=flush_config.min_memory_count,
            max_memory_age_hours=flush_config.max_memory_age_hours,
        )

        # åˆ›å»º MemoryExtractor
        memory_path = Path(self.app_config.memory.memory_dir)
        extractor = MemoryExtractor(
            model=flush_config.llm_model,
            llm_timeout=flush_config.llm_timeout,
        )

        # åˆ›å»º MemoryFlush
        return MemoryFlush(
            config=memory_flush_config,
            memory_path=memory_path,
            extractor=extractor,
        )

    def _init_memory_watcher(self) -> Optional["MemoryWatcherWrapper"]:
        """
        åˆå§‹åŒ– Memory Watcher

        Returns:
            MemoryWatcherWrapper å®ä¾‹ï¼Œå¦‚æœæœªå¯ç”¨åˆ™è¿”å› None
        """
        if not self.app_config.memory.enabled:
            return None

        watcher_config = self.app_config.memory.watcher
        if not watcher_config.enabled:
            return None

        # åˆ›å»º MemoryIndexer
        index_path = get_index_db_path()

        indexer = MemoryIndexer(
            db_path=index_path,
        )

        # åˆ›å»º watch è·¯å¾„åˆ—è¡¨
        watch_paths = [Path(p) for p in watcher_config.watch_paths]

        # åˆ›å»º MemoryWatcher
        watcher = MemoryWatcher(
            indexer=indexer,
            watch_paths=watch_paths,
            debounce_seconds=watcher_config.debounce_seconds,
        )

        # åˆ›å»ºåŒ…è£…å™¨
        wrapper = MemoryWatcherWrapper(
            watcher=watcher,
            check_interval=watcher_config.check_interval_seconds,
        )

        # å¯åŠ¨ç›‘å¬çº¿ç¨‹
        wrapper.start()

        return wrapper

    def _init_skill_loader(self) -> Optional[SkillLoader]:
        """
        åˆå§‹åŒ– Skills System Loader

        Returns:
            SkillLoader å®ä¾‹ï¼Œå¦‚æœ skills ç›®å½•ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        # å®šä¹‰ skills ç›®å½•
        skills_dirs = [
            Path("skills"),              # Project skills
            Path(".claude/skills"),      # User skills
        ]

        # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªç›®å½•å­˜åœ¨
        has_skills = any(d.exists() for d in skills_dirs)

        if not has_skills:
            logger.info("No skills directories found, skills system disabled")
            return None

        return SkillLoader(skills_dirs=skills_dirs)

    def _init_skill_tool(self) -> Optional[BaseTool]:
        """
        åˆå§‹åŒ– Skill meta-tool

        Returns:
            StructuredTool å®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰ skills åˆ™è¿”å› None
        """
        if self.skill_registry is None or self.skill_activator is None:
            return None

        try:
            return create_skill_tool(self.skill_registry, self.skill_activator)
        except Exception as e:
            logger.warning(f"Failed to create skill tool: {e}")
            return None

    def _build_skills_section(self) -> str:
        """
        æ„å»º Skills å‘ç°éƒ¨åˆ†ï¼Œç”¨äºç³»ç»Ÿæç¤ºè¯

        Returns:
            Skills å‘ç°éƒ¨åˆ†çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰ skills åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # Check if skill_registry exists and is initialized
        # Use getattr to handle cases where __init__ is not complete yet
        skill_registry = getattr(self, 'skill_registry', None)
        if skill_registry is None:
            return ""

        # è·å–æ ¼å¼åŒ–çš„ skills åˆ—è¡¨
        skills_list = skill_registry.get_formatted_skills_list()

        if not skills_list:
            return ""

        # ä½¿ç”¨ formatter æ„å»ºå®Œæ•´çš„ skills section
        formatter = SkillMessageFormatter()
        return formatter.format_skills_list_for_prompt(skills_list)

    def _handle_skill_activation_result(
        self,
        result: Dict[str, Any],
        conversation_id: str,
        config: RunnableConfig
    ) -> Optional[Dict[str, Any]]:
        """
        Handle skill activation result from tool call.

        When the activate_skill tool is called, it returns a SkillActivationResult.
        This method processes that result by:
        1. Injecting skill messages into the conversation
        2. Applying context modifiers (tool permissions, model override)
        3. Continuing the conversation with the skill active

        Args:
            result: The result dict from activate_skill tool
            conversation_id: Current conversation ID
            config: Current runnable config

        Returns:
            Updated result after processing, or None if no skill was activated
        """
        # Check if this is a skill activation result
        if not result.get("success") or "skill_name" not in result:
            return None

        skill_name = result["skill_name"]
        messages_data = result.get("messages", [])
        context_data = result.get("context_modifier", {})

        # Log skill activation
        logger.info(f"Processing skill activation: {skill_name}")

        # Apply context modifier
        context_modifier = ContextModifier(
            allowed_tools=context_data.get("allowed_tools"),
            model=context_data.get("model"),
            disable_model_invocation=context_data.get("disable_model_invocation", False)
        )
        self._apply_context_modifier(context_modifier, skill_name)

        # Inject messages into conversation
        if messages_data:
            self._inject_skill_messages(messages_data, conversation_id, config)

        # Store active skill context
        self._active_skill_context[skill_name] = context_modifier.to_dict()

        logger.info(
            f"Skill '{skill_name}' activated: "
            f"{len(messages_data)} messages injected, "
            f"context={context_modifier.to_dict()}"
        )

        return result

    def _inject_skill_messages(
        self,
        messages_data: List[Dict[str, Any]],
        conversation_id: str,
        config: RunnableConfig
    ) -> None:
        """
        Inject skill messages into the conversation.

        Args:
            messages_data: List of message dicts from skill activation
            conversation_id: Current conversation ID
            config: Current runnable config
        """
        # Get current conversation state
        state = self.agent.get_state(config)
        current_messages = list(state.messages.get("messages", []))

        # Convert and add each message
        for msg_data in messages_data:
            # Determine message type based on content
            if msg_data.get("isMeta") is True:
                # Hidden instruction message - create as AIMessage with metadata
                msg = AIMessage(
                    content=msg_data["content"],
                    additional_kwargs={"isMeta": True}
                )
            else:
                # Visible message - create as HumanMessage
                msg = HumanMessage(content=msg_data["content"])

            current_messages.append(msg)

        # Update state with new messages
        self.agent.update_state(config, {"messages": current_messages})

        logger.debug(f"Injected {len(messages_data)} messages into conversation {conversation_id}")

    def _apply_context_modifier(
        self,
        context_modifier: ContextModifier,
        skill_name: str
    ) -> None:
        """
        Apply context modifier from skill activation.

        This modifies the agent's execution context based on skill requirements:
        - allowed_tools: Pre-approve specific tools for this skill
        - model: Switch to a different model (if supported)
        - disable_model_invocation: Prevent skill from calling LLM

        Args:
            context_modifier: The context modifier to apply
            skill_name: Name of the skill being activated
        """
        # Always set the currently active skill
        self._active_skill_context["current_skill"] = skill_name

        # Apply tool permissions
        if context_modifier.allowed_tools is not None:
            logger.info(
                f"Skill '{skill_name}' granted access to: "
                f"{context_modifier.allowed_tools}"
            )
            # Store in active context for tool permission checks
            self._active_skill_context[f"{skill_name}_allowed_tools"] = context_modifier.allowed_tools

        # Apply model override
        if context_modifier.model is not None:
            logger.info(f"Skill '{skill_name}' requesting model: {context_modifier.model}")
            self._active_skill_context[f"{skill_name}_model"] = context_modifier.model
            # Attempt to switch model
            self._switch_model_for_skill(context_modifier.model, skill_name)

        # Apply model invocation disable
        if context_modifier.disable_model_invocation:
            logger.info(f"Skill '{skill_name}' has model invocation disabled")
            self._active_skill_context[f"{skill_name}_disable_model"] = True
            self._active_skill_context["disable_model_invocation"] = True

    def _check_tool_allowed(self, tool_name: str) -> bool:
        """
        Check if a tool is allowed to be used based on active skill context.

        Args:
            tool_name: Name of the tool to check

        Returns:
            True if tool is allowed, False otherwise
        """
        current_skill = self._active_skill_context.get("current_skill")
        if not current_skill:
            # No active skill, all tools are allowed
            return True

        allowed_tools_key = f"{current_skill}_allowed_tools"
        allowed_tools = self._active_skill_context.get(allowed_tools_key)

        if allowed_tools is None:
            # No restriction specified, all tools allowed
            return True

        # Check if tool is in the allowed list
        return tool_name in allowed_tools

    def _switch_model_for_skill(self, model: str, skill_name: str) -> bool:
        """
        Switch to a different model for the active skill.

        Args:
            model: Model identifier to switch to
            skill_name: Name of the skill requesting the switch

        Returns:
            True if model was switched, False otherwise
        """
        try:
            # Only switch if it's different from current model
            if self.config.model == model:
                logger.debug(f"Already using model {model}, no switch needed")
                return True

            logger.info(f"Switching model from {self.config.model} to {model} for skill '{skill_name}'")

            # Update the config
            self.config.model = model

            # Recreate the LLM instance
            self.llm = self._init_llm()

            # Recreate the agent with new LLM
            self.agent = self._create_agent()

            # Mark that we've switched models
            self._active_skill_context["model_switched"] = True
            self._active_skill_context["original_model"] = self.config.model

            logger.info(f"Successfully switched to model {model}")
            return True

        except Exception as e:
            logger.error(f"Failed to switch model to {model}: {e}")
            # Store the failure
            self._active_skill_context[f"{skill_name}_model_switch_failed"] = str(e)
            return False

    def _get_active_skill_model(self) -> Optional[str]:
        """
        Get the model requested by the active skill.

        Returns:
            Model identifier if a skill has requested a model switch, None otherwise
        """
        current_skill = self._active_skill_context.get("current_skill")
        if not current_skill:
            return None

        return self._active_skill_context.get(f"{current_skill}_model")

    def _is_model_invocation_disabled(self) -> bool:
        """
        Check if model invocation is disabled for the current skill.

        Returns:
            True if model invocation should be disabled, False otherwise
        """
        return self._active_skill_context.get("disable_model_invocation", False)

    def _get_total_tokens(self, result: Dict[str, Any]) -> int:
        """
        ä» Agent ç»“æœä¸­æå–æ€» token æ•° (å¢å¼ºç‰ˆ v2.1)

        v2.1 æ”¹è¿›:
        - ä¼˜å…ˆä½¿ç”¨ DynamicTokenCounter è¿›è¡Œç²¾ç¡®è®¡æ•°
        - ä¿ç•™ä» LLM å“åº” metadata æå–çš„åå¤‡æ–¹æ¡ˆ
        - æ”¯æŒè°ƒç”¨å‰é¢„è®¡æ•°å’Œè°ƒç”¨åéªŒè¯

        Args:
            result: Agent è¿”å›ç»“æœ

        Returns:
            æ€» token æ•°
        """
        total = 0

        # v2.1: ä¼˜å…ˆä½¿ç”¨ DynamicTokenCounter
        messages = result.get("messages", [])
        if messages:
            try:
                # ä½¿ç”¨æ–°çš„ token_counter ç²¾ç¡®è®¡æ•°
                counted = self.token_counter.count_messages(messages)
                if counted > 0:
                    return counted
            except Exception:
                pass  # é™çº§åˆ°æ—§æ–¹æ³•

        # åå¤‡æ–¹æ¡ˆï¼šä» LLM å“åº” metadata ä¸­è·å–
        if "response_metadata" in result:
            metadata = result["response_metadata"]
            if "usage" in metadata:
                usage = metadata["usage"]
                total += usage.get("input_tokens", 0)
                total += usage.get("output_tokens", 0)

        # åå¤‡æ–¹æ¡ˆï¼šä» messages ä¸­çš„ usage_metadata è·å–
        for msg in messages:
            if isinstance(msg, AIMessage):
                if hasattr(msg, "usage_metadata"):
                    usage = msg.usage_metadata or {}
                    total += usage.get("input_tokens", 0)
                    total += usage.get("output_tokens", 0)
                if hasattr(msg, "response_metadata"):
                    metadata = msg.response_metadata or {}
                    if "usage" in metadata:
                        usage = metadata["usage"]
                        total += usage.get("input_tokens", 0)
                        total += usage.get("output_tokens", 0)

        return total

    def _should_run_memory_flush(self, current_tokens: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿è¡Œ Memory Flush (Clawdbot é£æ ¼)

        v2.1 æ”¹è¿›:
        - ä½¿ç”¨ DynamicTokenCounter è¿›è¡Œæ›´ç²¾ç¡®çš„ token è®¡ç®—

        Args:
            current_tokens: å½“å‰ä½¿ç”¨çš„ token æ•°

        Returns:
            æ˜¯å¦åº”è¯¥è¿è¡Œ flush
        """
        if self.memory_flush is None:
            return False

        # è·å–é…ç½®
        flush_config = self.app_config.memory.flush
        context_window = self.app_config.llm.max_tokens  # å‡è®¾ä½¿ç”¨ max_tokens ä½œä¸ºä¸Šä¸‹æ–‡çª—å£
        soft_threshold = flush_config.soft_threshold_tokens
        reserve_tokens = flush_config.reserve_tokens_floor

        # è®¡ç®—é˜ˆå€¼ (Clawdbot: contextWindow - reserveTokensFloor - softThresholdTokens)
        threshold = context_window - reserve_tokens - soft_threshold

        # æ£€æŸ¥ token é˜ˆå€¼
        if current_tokens < threshold:
            return False

        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æœ¬æ¬¡ compaction ä¸­ flush è¿‡
        if (self.memory_flush_compaction_count is not None and
            self.memory_flush_compaction_count == self.compaction_count):
            return False

        return True

    def _check_and_flush(
        self,
        conversation_id: str,
        messages: List[BaseMessage],
        current_tokens: int,
    ) -> Optional[Dict[str, Any]]:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘ Memory Flush

        v2.1 æ”¹è¿›:
        - ä½¿ç”¨ DynamicTokenCounter ç²¾ç¡®è®¡ç®—æ¶ˆæ¯ token
        - ä¸ºåç»­å‹ç¼©æä¾›å‡†ç¡®çš„ token æ•°æ®

        Args:
            conversation_id: å¯¹è¯ ID
            messages: æ¶ˆæ¯åˆ—è¡¨
            current_tokens: å½“å‰ä½¿ç”¨çš„ token æ•°

        Returns:
            Flush ç»“æœï¼Œå¦‚æœæœªè§¦å‘åˆ™è¿”å› None
        """
        if self.memory_flush is None:
            return None

        # v2.1: ä½¿ç”¨ DynamicTokenCounter ç²¾ç¡®è®¡ç®—
        try:
            actual_tokens = self.token_counter.count_messages(messages)
        except Exception:
            actual_tokens = current_tokens  # é™çº§

        # å§‹ç»ˆæ›´æ–°æ¶ˆæ¯ç¼“å­˜ï¼ˆç§¯ç´¯ä¸Šä¸‹æ–‡ï¼‰
        for msg in messages:
            if isinstance(msg, HumanMessage):
                self.memory_flush.add_message("user", msg.content)
            elif isinstance(msg, AIMessage):
                self.memory_flush.add_message("assistant", msg.content)

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿è¡Œ flush
        if not self._should_run_memory_flush(actual_tokens):
            return None

        # è®°å½•å½“å‰ compaction_count
        self.memory_flush_compaction_count = self.compaction_count

        # æ£€æŸ¥å¹¶è§¦å‘ flush
        result = self.memory_flush.check_and_flush(actual_tokens)

        if result["flushed"]:
            # æ›´æ–° compaction è®¡æ•°
            self.compaction_count += 1
            # è®°å½•æ—¥å¿— (ç”¨æˆ·ä¸å¯è§)
            logger.info(
                f"Memory Flush triggered: {result['reason']}, "
                f"{result['memories_extracted']} memories extracted, "
                f"{result['memories_written']} memories written"
            )

            # v2.1: ä½¿ç”¨ AdvancedContextManager æ‰§è¡Œæ™ºèƒ½å‹ç¼©
            # è·å–å½“å‰çŠ¶æ€ä»¥è·å–å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
            config = {"configurable": {"thread_id": conversation_id}}
            state = self.agent.get_state(config)
            all_messages = list(state.messages.get("messages", [])) if state else []

            if all_messages:
                # æ™ºèƒ½å‹ç¼©åˆ° 50% å®¹é‡
                target_tokens = int(self.app_config.llm.max_tokens * 0.5)
                compressed = self.context_manager.compress(
                    all_messages,
                    target_tokens=target_tokens,
                    mode=CompressionMode.EXTRACT,
                )
                self.agent.update_state(config, {"messages": compressed})

                logger.info(
                    f"Context compressed (v2.1): {len(all_messages)} -> {len(compressed)} messages, "
                    f"tokens: {actual_tokens} -> {self.token_counter.count_messages(compressed)}"
                )

            # é‡ç½® token è®¡æ•°
            self.session_tokens = 0

        return result if result["flushed"] else None

    def _compact_conversation(
        self,
        conversation_id: str,
        keep_recent: int = 10
    ) -> bool:
        """
        å‹ç¼©å¯¹è¯å†å² (v2.1: ä½¿ç”¨ AdvancedContextManager)

        v2.1 æ”¹è¿›:
        - ä½¿ç”¨ AdvancedContextManager æ™ºèƒ½å‹ç¼©
        - æŒ‰ä¼˜å…ˆçº§ä¿ç•™æ¶ˆæ¯ (CRITICAL/HIGH/MEDIUM/LOW)
        - ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œå…³é”®ç”¨æˆ·æ¶ˆæ¯
        - ä»£æ›¿åŸæ¥çš„ç®€å•æˆªæ–­ (keep_recent=N)

        Args:
            conversation_id: å¯¹è¯ ID
            keep_recent: ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯æ•°é‡ (é™çº§é€‰é¡¹ï¼Œé»˜è®¤ 10 æ¡)

        Returns:
            æ˜¯å¦æˆåŠŸå‹ç¼©
        """
        try:
            config = {"configurable": {"thread_id": conversation_id}}
            state = self.agent.get_state(config)

            if not state or not state.messages:
                return False

            # è·å–å½“å‰æ‰€æœ‰æ¶ˆæ¯
            all_messages = list(state.messages.get("messages", []))

            if len(all_messages) <= keep_recent:
                # æ¶ˆæ¯æ•°é‡ä¸è¶³ï¼Œæ— éœ€å‹ç¼©
                return False

            # v2.1: ä½¿ç”¨ AdvancedContextManager æ™ºèƒ½å‹ç¼©
            target_tokens = int(self.app_config.llm.max_tokens * 0.5)  # å‹ç¼©åˆ° 50%
            compressed_messages = self.context_manager.compress(
                all_messages,
                target_tokens=target_tokens,
                mode=CompressionMode.EXTRACT,  # æ™ºèƒ½æå–ï¼Œéç®€å•æˆªæ–­
            )

            # æ›´æ–°çŠ¶æ€
            self.agent.update_state(config, {"messages": compressed_messages})

            logger.info(
                f"Conversation compacted (v2.1): {len(all_messages)} -> {len(compressed_messages)} "
                f"messages (target_tokens={target_tokens}, mode=EXTRACT)"
            )

            return True

        except Exception as e:
            logger.error(f"Conversation compaction failed: {e}")
            return False

    def _create_agent(self):
        """
        åˆ›å»º LangGraph Agentï¼ˆè‡ªå®šä¹‰ç‰ˆæœ¬ï¼Œæ”¯æŒç»“æ„åŒ–å“åº”ï¼‰

        ä½¿ç”¨ LangGraph çš„ StateGraph æ„å»ºè‡ªå®šä¹‰ Agentï¼š
        1. agent_node: LLM å†³ç­–èŠ‚ç‚¹ï¼ˆè°ƒç”¨å·¥å…·æˆ–è¿”å›ç»“æ„åŒ–å“åº”ï¼‰
        2. tool_node: å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
        3. æ¡ä»¶è¾¹ï¼šæ ¹æ®å“åº”ç±»å‹å†³å®šä¸‹ä¸€æ­¥

        Returns:
            Compiled LangGraph Agent
        """
        from langgraph.graph import END, StateGraph
        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
        from langgraph.prebuilt import ToolNode
        import json

        # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
        tool_node = ToolNode(self.tools)

        # å®šä¹‰ Agent çŠ¶æ€ç±»å‹
        class AgentState(TypedDict):
            messages: Sequence[BaseMessage]
            next: str  # "agent" æˆ– "end"

        def should_continue(state: AgentState) -> str:
            """
            å†³å®šä¸‹ä¸€æ­¥ï¼šç»§ç»­è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸ

            åŸºäº LLM çš„å“åº”åˆ¤æ–­ï¼š
            - å¦‚æœå“åº”åŒ…å«å·¥å…·è°ƒç”¨ â†’ è°ƒç”¨å·¥å…·
            - å¦‚æœå“åº”æ˜¯ç»“æ„åŒ– JSON (type=complete) â†’ ç»“æŸ
            """
            messages = state["messages"]
            last_message = messages[-1] if messages else None

            if not last_message or not isinstance(last_message, AIMessage):
                return "agent"

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆLangChain åŸç”Ÿæœºåˆ¶ï¼‰
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                return "tools"

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æ„åŒ–å“åº”ï¼ˆtype=completeï¼‰
            content = last_message.content
            if isinstance(content, str):
                # å°è¯•è§£æ JSON
                try:
                    # æå– JSON ä»£ç å—
                    import re
                    json_pattern = r'```json\s*\n(.*?)\n```'
                    matches = re.findall(json_pattern, content, re.DOTALL)
                    for match in matches:
                        data = json.loads(match.strip())
                        if data.get("action", {}).get("type") == "tool_call":
                            # éœ€è¦è°ƒç”¨å·¥å…· - è½¬æ¢ä¸º LangChain å·¥å…·è°ƒç”¨
                            return "convert_to_tool_call"
                        elif data.get("action", {}).get("type") == "complete":
                            # å®Œæˆï¼Œç»“æŸ
                            return "end"
                except (json.JSONDecodeError, KeyError):
                    pass

            # é»˜è®¤ç»§ç»­
            return "end"

        def convert_to_tool_call(state: AgentState) -> dict:
            """
            å°†ç»“æ„åŒ– JSON å“åº”è½¬æ¢ä¸º LangChain å·¥å…·è°ƒç”¨

            å½“æ¨¡å‹è¿”å› type="tool_call" çš„ JSON æ—¶ï¼Œè½¬æ¢ä¸ºå®é™…çš„å·¥å…·è°ƒç”¨
            """
            messages = list(state["messages"])
            last_message = messages[-1]

            if not isinstance(last_message, AIMessage):
                return {"messages": messages}

            content = last_message.content
            if not isinstance(content, str):
                return {"messages": messages}

            try:
                import re
                json_pattern = r'```json\s*\n(.*?)\n```'
                matches = re.findall(json_pattern, content, re.DOTALL)

                for match in matches:
                    data = json.loads(match.strip())
                    action = data.get("action", {})

                    if action.get("type") == "tool_call" and isinstance(action.get("content"), list):
                        # æå–å·¥å…·è°ƒç”¨åˆ—è¡¨
                        tool_calls_data = action["content"]

                        # æ„å»º tool_calls åˆ—è¡¨ï¼ˆLangChain æ ¼å¼ï¼‰
                        tool_calls = []
                        for tc_data in tool_calls_data:
                            tool_calls.append({
                                "name": tc_data["tool_name"],
                                "args": tc_data["arguments"],
                                "id": tc_data["tool_call_id"]
                            })

                        # åˆ›å»ºæ–°çš„ AIMessageï¼ŒåŒ…å«å·¥å…·è°ƒç”¨
                        new_message = AIMessage(
                            content="",  # å·¥å…·è°ƒç”¨æ—¶ content ä¸ºç©º
                            tool_calls=tool_calls
                        )

                        # æ›¿æ¢æœ€åä¸€æ¡æ¶ˆæ¯
                        messages[-1] = new_message
                        return {"messages": messages}

            except Exception as e:
                logger = __import__('logging').getLogger(__name__)
                logger.error(f"Failed to convert to tool call: {e}")

            return {"messages": messages}

        def call_model(state: AgentState) -> dict:
            """
            è°ƒç”¨ LLM è¿›è¡Œå†³ç­–

            è¾“å…¥åŒ…å«ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æ¶ˆæ¯
            """
            messages = list(state["messages"])

            # ç¡®ä¿ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯ç³»ç»Ÿæç¤ºè¯
            if not messages or not isinstance(messages[0], SystemMessage):
                messages.insert(0, SystemMessage(content=self.system_prompt))

            # è°ƒç”¨ LLM
            response = self.llm.invoke(messages)
            return {"messages": messages + [response]}

        # æ„å»ºå›¾
        workflow = StateGraph(AgentState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", tool_node)
        workflow.add_node("convert_to_tool_call", convert_to_tool_call)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("agent")

        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                "convert_to_tool_call": "convert_to_tool_call",
                "end": END
            }
        )

        # å·¥å…·æ‰§è¡Œåå›åˆ° agent
        workflow.add_edge("tools", "agent")

        # è½¬æ¢åå›åˆ° agentï¼ˆä¼šè§¦å‘å·¥å…·è°ƒç”¨ï¼‰
        workflow.add_edge("convert_to_tool_call", "agent")

        # ç¼–è¯‘å›¾
        app = workflow.compile()

        return app

    def add_tool(self, tool: BaseTool) -> None:
        """
        æ·»åŠ å·¥å…·åˆ° Agent

        Args:
            tool: å·¥å…·å®ä¾‹
        """
        self.tools.append(tool)
        # é‡æ–°åˆ›å»º Agent
        self.agent = self._create_agent()

    def add_tools(self, tools: List[BaseTool]) -> None:
        """
        æ‰¹é‡æ·»åŠ å·¥å…·åˆ° Agent

        Args:
            tools: å·¥å…·åˆ—è¡¨
        """
        self.tools.extend(tools)
        # é‡æ–°åˆ›å»º Agent
        self.agent = self._create_agent()

    def invoke(
        self,
        message: str,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
        config: Optional[RunnableConfig] = None,
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ Agent

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            conversation_id: å¯¹è¯ ID
            user_id: ç”¨æˆ· ID
            config: å¯é€‰çš„è¿è¡Œé…ç½®

        Returns:
            Agent å“åº”ç»“æœ
        """
        # ç”Ÿæˆ IDï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if conversation_id is None:
            conversation_id = f"conv_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        if user_id is None:
            user_id = "user_default"

        # å‡†å¤‡è¾“å…¥æ¶ˆæ¯
        messages = [HumanMessage(content=message)]

        # å‡†å¤‡é…ç½®
        if config is None:
            config = {}

        # æ·»åŠ çº¿ç¨‹ ID ç”¨äºè®°å¿†
        config["configurable"] = {"thread_id": conversation_id}

        # è°ƒç”¨ Agent
        try:
            result = self.agent.invoke(
                {"messages": messages},
                config,
            )

            # æ£€æŸ¥æ˜¯å¦æœ‰ skill æ¿€æ´»ç»“æœ
            skill_result = self._extract_skill_activation_result(result)
            if skill_result:
                # å¤„ç† skill æ¿€æ´»
                self._handle_skill_activation_result(
                    skill_result, conversation_id, config
                )

                # ç»§ç»­å¯¹è¯ï¼ˆskill æŒ‡ä»¤å·²æ³¨å…¥ï¼‰
                result = self.agent.invoke(
                    {"messages": []},  # ä¸æ·»åŠ æ–°æ¶ˆæ¯ï¼Œç»§ç»­ç°æœ‰å¯¹è¯
                    config,
                )

            # æå– token ä½¿ç”¨
            tokens_used = self._get_total_tokens(result)
            self.session_tokens += tokens_used

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ Memory Flush (é™é»˜æ‰§è¡Œï¼Œç”¨æˆ·ä¸å¯è§)
            flush_result = self._check_and_flush(
                conversation_id,
                result.get("messages", []),
                self.session_tokens,
            )

            # Flush ç»“æœä¸å¯¹ç”¨æˆ·æš´éœ²ï¼Œåªè®°å½•æ—¥å¿—
            # (Clawdbot é£æ ¼: é™é»˜è½®æ¬¡)

            # æå–å“åº”
            response = self._extract_response(result)

            return {
                "conversation_id": conversation_id,
                "user_id": user_id,
                "response": response,
                "success": True,
                "timestamp": datetime.now().isoformat(),
                "tokens_used": tokens_used,
                "session_tokens": self.session_tokens,
                # flush_triggered ä¸å†æš´éœ²ç»™ç”¨æˆ· (Clawdbot é£æ ¼: é™é»˜)
            }

        except Exception as e:
            return {
                "conversation_id": conversation_id,
                "user_id": user_id,
                "response": f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _extract_skill_activation_result(self, result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Extract skill activation result from agent response.

        Check if the agent called the activate_skill tool and extract the result.

        Args:
            result: Agent invoke result

        Returns:
            Skill activation result dict, or None if no skill was activated
        """
        messages = result.get("messages", [])

        for msg in reversed(messages):  # Check from most recent
            if isinstance(msg, AIMessage):
                # Check tool calls in the message
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if tool_call.get("name") == "activate_skill":
                            # Extract result from tool output
                            if hasattr(msg, 'additional_kwargs'):
                                additional = msg.additional_kwargs or {}
                                if "tool_output" in additional:
                                    import json
                                    try:
                                        return json.loads(additional["tool_output"])
                                    except json.JSONDecodeError:
                                        pass

                # Also check content for skill activation result
                if isinstance(msg.content, str):
                    import json
                    try:
                        content = json.loads(msg.content)
                        if isinstance(content, dict) and "skill_name" in content:
                            return content
                    except json.JSONDecodeError:
                        pass

        return None

    def _extract_response(self, result: Dict[str, Any]) -> str:
        """
        ä» Agent ç»“æœä¸­æå–å“åº”æ–‡æœ¬

        æ”¯æŒç»“æ„åŒ– JSON å“åº”æ ¼å¼çš„è§£æ

        Args:
            result: Agent è¿”å›ç»“æœ

        Returns:
            å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæ˜¯ç»“æ„åŒ– JSONï¼Œåˆ™æå–å¹¶è¿”å›æ˜¾ç¤ºå†…å®¹ï¼‰
        """
        messages = result.get("messages", [])
        if not messages:
            return ""

        # è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                content = msg.content
                if isinstance(content, str):
                    # å°è¯•è§£æç»“æ„åŒ–å“åº”
                    parsed = self._try_parse_structured_response(content)
                    if parsed:
                        return parsed
                    return content
                elif isinstance(content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                    text_parts = [
                        part.get("text", "")
                        for part in content
                        if isinstance(part, dict) and "text" in part
                    ]
                    combined = "\n".join(text_parts)
                    # å°è¯•è§£æç»“æ„åŒ–å“åº”
                    parsed = self._try_parse_structured_response(combined)
                    if parsed:
                        return parsed
                    return combined

        return ""

    def _try_parse_structured_response(self, content: str) -> Optional[str]:
        """
        å°è¯•è§£æç»“æ„åŒ–å“åº”

        Args:
            content: åŸå§‹å“åº”å†…å®¹

        Returns:
            è§£æåçš„æ˜¾ç¤ºå†…å®¹ï¼Œå¦‚æœä¸æ˜¯ç»“æ„åŒ–å“åº”åˆ™è¿”å› None
        """
        try:
            from backend.models.response import parse_structured_response

            structured = parse_structured_response(content)
            if structured and structured.is_complete():
                # æ˜¯ç»“æ„åŒ–çš„ complete å“åº”ï¼Œæå–æ˜¾ç¤ºå†…å®¹
                final_report = structured.get_final_report()

                # æ„å»ºæ˜¾ç¤ºå†…å®¹ï¼ˆä¸ ba_agent.py ä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
                display_parts = []

                # æ€ç»´é“¾åˆ†æï¼ˆå¯æŠ˜å ï¼‰
                if structured.task_analysis:
                    display_parts.append(f"""
<div class="task-analysis" style="margin-bottom: 12px; padding: 10px; background: #f0f7ff; border-left: 3px solid #2196F3; border-radius: 4px;">
    <details>
        <summary style="cursor: pointer; font-weight: 500; color: #1976D2;">ğŸ’¡ æ€ç»´é“¾åˆ†æ</summary>
        <div style="margin-top: 8px; font-size: 13px; color: #555; white-space: pre-wrap;">{structured.task_analysis}</div>
    </details>
</div>
""")

                # æ‰§è¡Œè®¡åˆ’
                if structured.execution_plan:
                    display_parts.append(f"""
<div class="execution-plan" style="margin-bottom: 12px; padding: 10px; background: #fff3e0; border-left: 3px solid #FF9800; border-radius: 4px;">
    <div style="font-weight: 500; color: #E65100; margin-bottom: 4px;">ğŸ“‹ æ‰§è¡Œè®¡åˆ’</div>
    <div style="font-size: 13px; color: #555;">{structured.execution_plan}</div>
</div>
""")

                # æœ€ç»ˆæŠ¥å‘Š
                has_html = '<div' in final_report or '<script' in final_report or 'echarts' in final_report.lower()

                if has_html:
                    display_parts.append(f'<div class="final-report">{final_report}</div>')
                else:
                    display_parts.append(f'<div class="final-report" style="line-height: 1.6;">{final_report.replace("\\n", "<br>")}</div>')

                # æ¨èé—®é¢˜
                if structured.action.recommended_questions:
                    questions_html = '<br>'.join(
                        f'<button class="recommended-question" style="display: block; width: 100%; text-align: left; padding: 10px; margin: 6px 0; background: #f5f5f5; border: 1px solid #ddd; border-radius: 6px; cursor: pointer;">ğŸ’¡ {q}</button>'
                        for q in structured.action.recommended_questions
                    )
                    display_parts.append(f"""
<div class="recommended-questions" style="margin-top: 16px; padding: 12px; background: #f9f9f9; border-radius: 6px;">
    <div style="font-weight: 500; color: #333; margin-bottom: 8px;">ğŸ¤” æ¨èé—®é¢˜</div>
    {questions_html}
</div>
""")

                # ä¸‹è½½é“¾æ¥
                if structured.action.download_links:
                    links_html = '<br>'.join(
                        f'<a href="/api/v1/files/download/{filename}" style="display: inline-block; padding: 8px 16px; margin: 4px; background: #4CAF50; color: white; text-decoration: none; border-radius: 4px;">ğŸ“¥ {filename}</a>'
                        for filename in structured.action.download_links
                    )
                    display_parts.append(f"""
<div class="download-links" style="margin-top: 12px; padding: 12px; background: #e8f5e9; border-radius: 6px;">
    <div style="font-weight: 500; color: #2E7D32; margin-bottom: 8px;">ğŸ“¦ å¯ä¸‹è½½æ–‡ä»¶</div>
    {links_html}
</div>
""")

                result = "\n".join(display_parts)

                # æ·»åŠ  HTML æ ‡è®°
                if has_html:
                    result = f"<!-- HAS_HTML -->{result}"

                return result

        except Exception as e:
            logger = __import__('logging').getLogger(__name__)
            logger.debug(f"Failed to parse structured response: {e}")

        return None

    def stream(
        self,
        message: str,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
        config: Optional[RunnableConfig] = None,
    ):
        """
        æµå¼è°ƒç”¨ Agent

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            conversation_id: å¯¹è¯ ID
            user_id: ç”¨æˆ· ID
            config: å¯é€‰çš„è¿è¡Œé…ç½®

        Yields:
            Agent å“åº”ç‰‡æ®µ
        """
        # ç”Ÿæˆ IDï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if conversation_id is None:
            conversation_id = f"conv_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        if user_id is None:
            user_id = "user_default"

        # å‡†å¤‡è¾“å…¥æ¶ˆæ¯
        messages = [HumanMessage(content=message)]

        # å‡†å¤‡é…ç½®
        if config is None:
            config = {}

        # æ·»åŠ çº¿ç¨‹ ID ç”¨äºè®°å¿†
        config["configurable"] = {"thread_id": conversation_id}

        # æµå¼è°ƒç”¨ Agent
        try:
            for chunk in self.agent.stream(
                {"messages": messages},
                config,
            ):
                yield chunk

        except Exception as e:
            yield {
                "error": str(e),
                "success": False,
            }

    def get_conversation_history(
        self,
        conversation_id: str,
    ) -> List[BaseMessage]:
        """
        è·å–å¯¹è¯å†å²

        Args:
            conversation_id: å¯¹è¯ ID

        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        # ä»æ£€æŸ¥ç‚¹è·å–å†å²
        config = {"configurable": {"thread_id": conversation_id}}
        state = self.agent.get_state(config)

        return state.messages.get("messages", [])

    def reset_conversation(self, conversation_id: str) -> None:
        """
        é‡ç½®å¯¹è¯å†å²

        Args:
            conversation_id: å¯¹è¯ ID
        """
        # åˆ é™¤æ£€æŸ¥ç‚¹ä¸­çš„å¯¹è¯
        config = {"configurable": {"thread_id": conversation_id}}
        self.agent.update_state(config, {"messages": []})

    def shutdown(self) -> None:
        """
        å…³é—­ Agentï¼Œé‡Šæ”¾èµ„æº

        åœæ­¢ MemoryWatcher çº¿ç¨‹
        """
        if self.memory_watcher:
            self.memory_watcher.stop()
            self.memory_watcher = None


class MemoryWatcherWrapper:
    """
    Memory Watcher åŒ…è£…å™¨

    åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ MemoryWatcherï¼Œå®šæœŸæ£€æŸ¥æ–‡ä»¶å˜æ›´
    """

    def __init__(
        self,
        watcher: MemoryWatcher,
        check_interval: float = 5.0,
    ):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨

        Args:
            watcher: MemoryWatcher å®ä¾‹
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        self.watcher = watcher
        self.check_interval = check_interval
        self._thread: Optional[threading.Thread] = None
        self._running = False
        self._stop_event = threading.Event()

    def _watch_loop(self) -> None:
        """ç›‘å¬å¾ªç¯"""
        import logging
        logger = logging.getLogger(__name__)

        while not self._stop_event.is_set():
            try:
                # å¤„ç†å˜æ›´
                results = self.watcher.process_changes()

                if results["processed"] > 0 or results["failed"] > 0:
                    logger.info(
                        f"MemoryWatcher: å¤„ç†äº† {results['processed']} ä¸ªæ–‡ä»¶å˜æ›´"
                    )

                    if results["failed"] > 0:
                        logger.warning(
                            f"MemoryWatcher: {results['failed']} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥"
                        )

            except Exception as e:
                logger.error(f"MemoryWatcher å¾ªç¯é”™è¯¯: {e}")

            # ç­‰å¾…æŒ‡å®šé—´éš”æˆ–åœæ­¢äº‹ä»¶
            self._stop_event.wait(self.check_interval)

    def start(self) -> None:
        """å¯åŠ¨ç›‘å¬çº¿ç¨‹"""
        if self._running:
            return

        self._running = True
        self._stop_event.clear()
        self.watcher.start()

        self._thread = threading.Thread(
            target=self._watch_loop,
            daemon=True,
            name="MemoryWatcher",
        )
        self._thread.start()

    def stop(self) -> None:
        """åœæ­¢ç›‘å¬çº¿ç¨‹"""
        if not self._running:
            return

        self._running = False
        self._stop_event.set()

        if self._thread:
            self._thread.join(timeout=2.0)
            self._thread = None

        self.watcher.stop()

    def is_running(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return self._running and self._thread is not None and self._thread.is_alive()


def create_agent(
    tools: Optional[List[BaseTool]] = None,
    system_prompt: Optional[str] = None,
) -> BAAgent:
    """
    åˆ›å»º BA-Agent å®ä¾‹çš„ä¾¿æ·å‡½æ•°

    Args:
        tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

    Returns:
        BA-Agent å®ä¾‹
    """
    return BAAgent(tools=tools, system_prompt=system_prompt)


# å¯¼å‡º
__all__ = [
    "BAAgent",
    "AgentState",
    "create_agent",
]
